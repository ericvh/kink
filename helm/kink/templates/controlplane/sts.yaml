{{- $k8sBin := "k3s" }}
{{- $dataDir := "/var/lib/rancher/k3s" }}
{{- $tokenVar := "K3S_TOKEN" }}
{{- if .Values.rke2.enabled }}
{{- $k8sBin = "rke2" }}
{{- $dataDir = "/var/lib/rancher/rke2" }}
{{- $tokenVar = "RKE2_TOKEN" }}
{{- end }}
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {{ include "kink.controlplane.fullname" . }}
  labels:
    {{- include "kink.controlplane.labels" . | nindent 4 }}
spec:
  serviceName: {{ include "kink.controlplane.fullname" . }}
  podManagementPolicy: "Parallel"
  replicas: {{ .Values.controlplane.replicaCount }}
  selector:
    matchLabels:
      {{- include "kink.controlplane.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      {{- with .Values.controlplane.podAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      labels:
        {{- include "kink.controlplane.selectorLabels" . | nindent 8 }}
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include "kink.controlplane.serviceAccountName" . }}
      securityContext:
        {{- toYaml .Values.controlplane.podSecurityContext | nindent 8 }}
      {{- if false }}
      initContainers:
        - name: create-token-secret
          securityContext:
            {{- toYaml .Values.controlplane.securityContext | nindent 12 }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: '{{ $tokenVar }}'
            valueFrom:
              secretKeyRef:
                {{- if .Values.token.existingSecret.name }}
                name: {{ .Values.token.existingSecret.name }}
                key:  {{ .Values.token.existingSecret.key }}
                {{- else }}
                name: {{ include "kink.fullname" . }}
                key: token
                {{- end }}
          command: [sh,-ce]
          args:
          - |
            {{- if gt (int .Values.controlplane.replicaCount) 1 }}
            if [ "${POD_NAME}" = '{{ include "kink.controlplane.fullname" . }}-0' ]; then
              exec '{{ $k8sBin }}' server \
                '--data-dir={{ $dataDir }}' \
                --cluster-init
            else 
              exec '{{ $k8sBin }}' server \
                '--data-dir={{ $dataDir }}' \
                --server={{ include "kink.controlplane.url" . }}
            fi
            {{- else }}
            '{{ $k8sBin }}' server \
              '--data-dir={{ $dataDir }}' \
              '--tls-san={{ .Release.Name }}' \
              '--tls-san={{ include "kink.controlplane.fullname" . }}' \
              '--tls-san={{ include "kink.controlplane.fullname" . }}.{{ .Release.Namespace }}' \
              '--tls-san={{ include "kink.controlplane.fullname" . }}.{{ .Release.Namespace }}.svc' \
              '--tls-san={{ include "kink.controlplane.fullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.clusterDomain }}' \
          - '--tls-san=$(POD_NAME).{{ include "kink.controlplane.fullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.clusterDomain }}'
            {{- end }} &
            RKE_PID=$!
            while ! [ -f '{{ $dataDir }}/server/node-token' ]; then
              echo 'Waiting for node-token to exist'
              sleep 10;
            done
            kill -s INT "${RKE_PID}"
            k3s kubectl create secret generic '{{ include "kink.fullname" . }}-node-token' --from-file '{{ $tokenVar }}={{ $dataDir }}/server/node-token'
          command: [ '{{ $k8sBin }}' ]
          args:

      {{- end }}
      containers:
        - name: {{ .Chart.Name }}
          securityContext:
            {{- toYaml .Values.controlplane.securityContext | nindent 12 }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: '{{ $tokenVar }}'
            valueFrom:
              secretKeyRef:
                {{- if .Values.token.existingSecret.name }}
                name: {{ .Values.token.existingSecret.name }}
                key:  {{ .Values.token.existingSecret.key }}
                {{- else }}
                name: {{ include "kink.fullname" . }}
                key: token
                {{- end }}
          {{- if gt (int .Values.controlplane.replicaCount) 1 }}
          command: [sh,-ce]
          args:
          - |
            if [ "${POD_NAME}" = '{{ include "kink.controlplane.fullname" . }}-0' ]; then
              exec '{{ $k8sBin }}' server \
                '--data-dir={{ $dataDir }}' \
                --cluster-init
            else 
              exec '{{ $k8sBin }}' server \
                '--data-dir={{ $dataDir }}' \
                --server={{ include "kink.controlplane.url" . }}
           fi
          {{- else }}
          command: [ '{{ $k8sBin }}' ]
          args:
          - server
          - '--data-dir={{ $dataDir }}'
          - '--tls-san={{ .Release.Name }}'
          - '--tls-san={{ include "kink.controlplane.fullname" . }}'
          - '--tls-san={{ include "kink.controlplane.fullname" . }}.{{ .Release.Namespace }}'
          - '--tls-san={{ include "kink.controlplane.fullname" . }}.{{ .Release.Namespace }}.svc'
          - '--tls-san={{ include "kink.controlplane.fullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.clusterDomain }}'
          - '--tls-san=$(POD_NAME).{{ include "kink.controlplane.fullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.clusterDomain }}'
          {{- end }}
          ports:
            - name: api
              containerPort: 6443
              protocol: TCP
            - name: kubelet-metrics
              containerPort: 10250
              protocol: TCP
            - name: etcd-client
              containerPort: 2379
              protocol: TCP
            - name: etcd-peer
              containerPort: 2380
              protocol: TCP
            {{- if .Values.rke2.enabled }}
            - name: rke2-discover
              containerPort: 9345
              protocol: TCP
            {{- end }}

          startupProbe:
            tcpSocket:
              port: api
            # TODO: Health checks w/ authentication
            # httpGet:
            #   path: /livez
            #   port: api
            #   scheme: HTTPS
            # exec:
            #   command:
            #   - k3s
            #   - kubectl
            #   - --kubeconfig=/etc/rancher/k3s/k3s.yaml
            #   - version
            failureThreshold: 20
            periodSeconds: 15
          livenessProbe:
            # httpGet:
            #   path: /livez
            #   scheme: HTTPS
            #   port: api
            # exec:
            #  command:
            #  - k3s
            #  - kubectl
            #  - --kubeconfig=/etc/rancher/k3s/k3s.yaml
            #  - version
            tcpSocket:
              port: api
          readinessProbe:
            #httpGet:
            #  path: /readyz
            #  scheme: HTTPS
            #  port: api
                #  exec:
                # command:
                # - k3s
                # - kubectl
                # - --kubeconfig=/etc/rancher/k3s/k3s.yaml
                # - version
            tcpSocket:
              port: api
          resources:
            {{- toYaml .Values.controlplane.resources | nindent 12 }}
          volumeMounts:
          - name: data
            mountPath: '{{ $dataDir }}'
            subPath: '{{ $dataDir | trimPrefix "/" }}'
          - name: data
            mountPath: /etc/rancher
            subPath: etc/rancher
          - name: kubelet
            mountPath: /var/lib/kubelet
            subPath: var/lib/rancher
          {{- range .Values.controlplane.persistence.extraMounts }}
          - name: data
            mountPath: /{{ . }}
            subPath: {{ . | trimPrefix "/" }}
          {{- end }}

      {{- with .Values.controlplane.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.controlplane.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.controlplane.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      volumes:
      {{- if not .Values.kubelet.persistence.enabled }}
      - name: kubelet
        emptyDir: {}
      {{- end }}
      {{- if not .Values.controlplane.persistence.enabled }}
      - name: data
        emptyDir: {}
      {{- end }}
  volumeClaimTemplates:
  {{- if .Values.controlplane.persistence.enabled }}
  - metadata:
      name: data
    spec:
      accessModes: {{ .Values.controlplane.persistence.accessModes | toJson }}
      {{- with .Values.controlplane.persistence.storageClassName }}
      storageClassName: '{{ . }}'
      {{- end }}
      resources:
        requests:
          storage: {{ .Values.controlplane.persistence.size }}
  {{- end }}
  {{- if .Values.kubelet.persistence.enabled }}
  - metadata:
      name: kubelet
    spec:
      accessModes: {{ .Values.kubelet.persistence.accessModes | toJson }}
      {{- with .Values.kubelet.persistence.storageClassName }}
      storageClassName: '{{ . }}'
      {{- end }}
      resources:
        requests:
          storage: {{ .Values.kubelet.persistence.size }}
  {{- end }}

